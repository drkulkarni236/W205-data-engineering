{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Report\n",
    "\n",
    "## Summary\n",
    "\n",
    "This project uses kafka to consume messages in a batch fashion from a given json file. Spark is then used to consume and transform those messages and unroll them appropriately into a format that can be soted in HDFS and queried using spark sql."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the pipeline\n",
    "\n",
    "The .yml file contains information required to spin up each of the containers. The first image is a zookeeper image that works with kafka to track the status of nodes in the Kafka cluster and maintain a list of Kafka topics and messages. The second image is kafka, which is the broker that can receive messages produced by a producer and store them in a topic. We use only 1 broker, 1 partition and no replications for this image. We chose the topic name assessments because the data is about assessments.\n",
    "\n",
    "The next image is cloudera which is ecosystem that let's us use hadoop and HDFS to store the data. We use Spark as the service to consume the messages, and it depends on the cloudera image so that we can save data in HDFS. We expose port 8889 so we can connect to it later for running our notebook. We use spark to consume messages from kafka, unroll the nested json data using a forced schema and store it into HDFS as parquet files. We can then use spark sql to query the data. The last container is the MIDS container which is a ubuntu machine to mirror the host machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Data\n",
    "\n",
    "The data is provided in a form of a json file and we consume it at once in a batch fashion. The data dscribes a series of assessments taken by users on various topics. It is in a nested form with general information about the assessment and the user on the first level. The sequences field on the first level containts 4 more keys. One of the keys is questions. The questions key contains a value of each of the questions in that particular assessment. Each assessment has a different number of questions, which makes unrolling this json file more difficult. There are ways to unroll it using ArrayType when forcing the schema, but for our purposes the information under questions is not required, so I have not attempted to do that. Each question further has keys for all the options for that question and other information related to how the user asnwered that question. The other key under sequences is counts which contains all the useful information regarding the users performance in the assessment.\n",
    "\n",
    "The questions that are answered using the queries at the end of this report ar\n",
    "\n",
    "1. What is the count of assessments?\n",
    "2. Which are the most common courses taken?3Which are the least common courses taken?\n",
    "3. Which are the least common courses taken?\n",
    "4. How many assessments have all questions answered correctly by the users and how many are not?\n",
    "5. What is the distribution of questions per assessment?\n",
    "6. What courses have the highest scores by the user who took assessments in that course?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant portion of the history file\n",
    "\n",
    "The commands used to create the pipeline are shown below. Note that I restarted my virtual machine just before running these, so the command history from earlier in the semester was lost. \n",
    "\n",
    "\n",
    "    16  docker-compose up -d\n",
    "    17  docker-compose logs zookeeper | grep -i binding\n",
    "    18  docker-compose logs kafka | grep -i started\n",
    "    19  docker-compose ps\n",
    "    20  docker-compose logs kafka\n",
    "    22  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "    23  docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "    24  docker-compose exec mids bash -c \"cat /w205/project-2-drkulkarni236/assessment-attempts-20180128-121051-nested.json | jq \".[]\" -c | kafkacat -P -b kafka:29092 -t assessments \"\n",
    "    25  docker-compose exec kafka kafka-console-consumer --bootstrap-server localhost:29092 --topic assessments --from-beginning\n",
    "    27  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
    "    29  history > drkulkarni236-history.txt\n",
    "    \n",
    "    \n",
    "Following is a brief description of what the commands are doing.\n",
    "\n",
    "- Line 16 - We first spin up our containers using docer-compose according to the docker-compose.yml file. \n",
    "- Line 17-20 - Sanity checks to make sure all ther services are up.\n",
    "- Line 22 - Create a kafka topic, i.e. the feed name to wchi hthe records are stored and published.\n",
    "- Line 23 - Sanity check to make sure kafka topic is created.\n",
    "- Line 24 - Execute through mids bash, open the data from the json file, pipe it to jq to convert it into different messages (because kafka needs a newline at the end of every message), -c to make the messages compact, use kafkacat as producer (-P) with particular broker (-b) and topic name (-t) to produce the messages.\n",
    "- Line 25, 27 - Sanity check, consume messages to make sure they were produced properly.\n",
    "- Line 29 - Save all the command to history file. \n",
    "\n",
    "The last command to open this notebook is missing from the history file as it was run after creating the history file.\n",
    "    \n",
    "    docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8889 --ip 0.0.0.0 --allow-root --notebook-dir=/w205/' pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spark to unroll .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read raw data from the kafka topic\n",
    "raw_assessments = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"assessments\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache the data\n",
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cast the data as string because kafka sends the data in binary fromat\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the assessments object\n",
    "assessments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":true,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f106765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first message\n",
    "assessments.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write raw data in parquet format\n",
    "assessments.write.parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from parquet\n",
    "read_assessments = spark.read.parquet('/tmp/assessments')\n",
    "read_assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Infered Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "# Convert to data frame\n",
    "extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the data frame, the data is not unrolled properly due to the nested nature of the json file\n",
    "extracted_assessments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at the schema, it's all messed up\n",
    "extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We look at the json file and force a schema that makes sense.\n",
    "final_schema = StructType([StructField('keen_timestamp', StringType(), True),\n",
    "                     StructField('max_attempts', StringType(), True),\n",
    "                     StructField('started_at', StringType(), True),\n",
    "                     StructField('base_exam_id', StringType(), True),\n",
    "                     StructField('user_exam_id', StringType(), True),\n",
    "                     StructField('keen_created_at', StringType(), True),\n",
    "                     StructField('certification', StringType(), True),\n",
    "                     StructField('keen_id', StringType(), True),\n",
    "                     StructField('exam_name', StringType(), True),\n",
    "                     StructField('sequences', StructType([\n",
    "                         StructField('attempt', StringType(), True),\n",
    "                         StructField('id', StringType(), True),\n",
    "                         StructField('questions', StringType(), True),                          \n",
    "                         StructField('counts', StructType([\n",
    "                             StructField('incomplete', StringType(), True),\n",
    "                             StructField('submitted', StringType(), True),\n",
    "                             StructField('incorrect', StringType(), True),\n",
    "                             StructField('all_correct', StringType(), True),\n",
    "                             StructField('correct', StringType(), True),\n",
    "                             StructField('total', StringType(), True),\n",
    "                             StructField('unanswered', StringType(), True)\n",
    "                         ]))]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert to data frame using the forced schema\n",
    "focused_extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF(schema=final_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+--------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|    keen_timestamp|max_attempts|          started_at|        base_exam_id|        user_exam_id|   keen_created_at|certification|             keen_id|           exam_name|           sequences|\n",
      "+------------------+------------+--------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+--------------------+\n",
      "| 1516717442.735266|         1.0|2018-01-23T14:23:...|37f0a30a-7464-11e...|6d4089e4-bde5-4a2...| 1516717442.735266|        false|5a6745820eb8ab000...|Normal Forms and ...|[1,5b28a462-7a3b-...|\n",
      "| 1516717377.639827|         1.0|2018-01-23T14:21:...|37f0a30a-7464-11e...|2fec1534-b41f-441...| 1516717377.639827|        false|5a674541ab6b0a000...|Normal Forms and ...|[1,5b28a462-7a3b-...|\n",
      "| 1516738973.653394|         1.0|2018-01-23T20:22:...|4beeac16-bb83-4d5...|8edbc8a8-4d26-429...| 1516738973.653394|        false|5a67999d3ed3e3000...|The Principles of...|[1,b370a3aa-bf9e-...|\n",
      "|1516738921.1137421|         1.0|2018-01-23T20:21:...|4beeac16-bb83-4d5...|c0ee680e-8892-4e6...|1516738921.1137421|        false|5a6799694fc7c7000...|The Principles of...|[1,b370a3aa-bf9e-...|\n",
      "| 1516737000.212122|         1.0|2018-01-23T19:48:...|6442707e-7488-11e...|e4525b79-7904-405...| 1516737000.212122|        false|5a6791e824fccd000...|Introduction to B...|[1,04a192c1-4f5c-...|\n",
      "+------------------+------------+--------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting one element, it makes more sense now\n",
    "focused_extracted_assessments.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- sequences: struct (nullable = true)\n",
      " |    |-- attempt: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- questions: string (nullable = true)\n",
      " |    |-- counts: struct (nullable = true)\n",
      " |    |    |-- incomplete: string (nullable = true)\n",
      " |    |    |-- submitted: string (nullable = true)\n",
      " |    |    |-- incorrect: string (nullable = true)\n",
      " |    |    |-- all_correct: string (nullable = true)\n",
      " |    |    |-- correct: string (nullable = true)\n",
      " |    |    |-- total: string (nullable = true)\n",
      " |    |    |-- unanswered: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the force schema, it looks like what we want\n",
    "focused_extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write filtered data in parquet format\n",
    "focused_extracted_assessments.write.parquet(\"/tmp/focussed_assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create temp table using the filtered data\n",
    "focused_extracted_assessments.registerTempTable('focused_assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3280|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of assessments\n",
    "spark.sql(\"select count(*) from focused_assessments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           exam_name|user_counts|\n",
      "+--------------------+-----------+\n",
      "|        Learning Git|        394|\n",
      "|Introduction to P...|        162|\n",
      "|Intermediate Pyth...|        158|\n",
      "|Introduction to J...|        158|\n",
      "|Learning to Progr...|        128|\n",
      "|Introduction to M...|        119|\n",
      "|Software Architec...|        109|\n",
      "|Beginning C# Prog...|         95|\n",
      "|    Learning Eclipse|         85|\n",
      "|Learning Apache M...|         80|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which are the most common courses taken?\n",
    "spark.sql(\"select exam_name, count(user_exam_id) as user_counts from focused_assessments \\\n",
    "          group by exam_name order by user_counts DESC limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           exam_name|user_counts|\n",
      "+--------------------+-----------+\n",
      "|Nulls, Three-valu...|          1|\n",
      "|Learning to Visua...|          1|\n",
      "|Native Web Apps f...|          1|\n",
      "|Operating Red Hat...|          1|\n",
      "|Client-Side Data ...|          2|\n",
      "|Arduino Prototypi...|          2|\n",
      "|What's New in Jav...|          2|\n",
      "|Understanding the...|          2|\n",
      "|Hibernate and JPA...|          2|\n",
      "|Learning Spring P...|          2|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which are the least common courses taken?\n",
    "spark.sql(\"select exam_name, count(user_exam_id) as user_counts from focused_assessments\\\n",
    "          group by exam_name order by user_counts limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|count(1)|all_correct|\n",
      "+--------+-----------+\n",
      "|       5|       null|\n",
      "|    2434|      false|\n",
      "|     841|       true|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many assessments have all questions answered correctly by the users and how many are not?\n",
    "spark.sql(\"select count(*), sequences.counts.all_correct from focused_assessments \\\n",
    "          group by sequences.counts.all_correct\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|count(1)|total_questions|\n",
      "+--------+---------------+\n",
      "|       5|           null|\n",
      "|      56|              1|\n",
      "|      21|              2|\n",
      "|     239|              3|\n",
      "|    1646|              4|\n",
      "|     940|              5|\n",
      "|     122|              6|\n",
      "|     177|              7|\n",
      "|      59|              8|\n",
      "|      14|             10|\n",
      "|       1|             20|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the distribution of questions per assessment?\n",
    "spark.sql(\"select count(*), CAST(sequences.counts.total as int) as total_questions from focused_assessments \\\n",
    "          group by total_questions order by total_questions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|           exam_name|avg_correct_percent|\n",
      "+--------------------+-------------------+\n",
      "|The Closed World ...|              100.0|\n",
      "|Nulls, Three-valu...|              100.0|\n",
      "|Learning to Visua...|              100.0|\n",
      "|Learning SQL for ...|  97.72727272727273|\n",
      "|Introduction to J...|  87.59493670886076|\n",
      "|Introduction to A...|  83.33333333333333|\n",
      "|Introduction to A...|  83.33333333333333|\n",
      "|Getting Ready for...|               80.0|\n",
      "|Cloud Native Arch...|               80.0|\n",
      "|Understanding the...|  78.57142857142857|\n",
      "|Introduction to A...|  76.92307692307692|\n",
      "|Beginning Program...|  76.58227848101266|\n",
      "|Learning Apache H...|            76.5625|\n",
      "|Refactor a Monoli...|  76.47058823529412|\n",
      "|Starting a Grails...|               75.0|\n",
      "|Using Storytellin...|               75.0|\n",
      "|Introduction to H...|               75.0|\n",
      "|Git Fundamentals ...|               75.0|\n",
      "|   Python Epiphanies|  74.18300653594771|\n",
      "|Mastering Python ...|               74.0|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What courses have the highest scores by the user who took assessments in that course?\n",
    "spark.sql(\"select exam_name, avg(percent_correct) as avg_correct_percent from \\\n",
    "          (select exam_name, 100*CAST(sequences.counts.correct as int)/CAST(sequences.counts.total as int) \\\n",
    "         as percent_correct from focused_assessments)t \\\n",
    "          group by exam_name order by avg_correct_percent DESC\").show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
